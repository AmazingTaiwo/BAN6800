{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "114d2a75-8221-4659-b574-9fed3aebb1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LOAD SILVER TABLES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "po_silver = spark.table(\"abc.abc_dw_silver.abc_dw_sl_pur_ord\")\n",
    "pr_silver = spark.table(\"abc.abc_dw_silver.abc_dw_sl_pr_req\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FULL OUTER JOIN PR + PO DATASETS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Rename lastchangedatetime in both tables before join to avoid duplicate column names\n",
    "#po_silver = po_silver.withColumnRenamed(\"lastchangedatetime\", \"po_lastchangedatetime\")\n",
    "#pr_silver = pr_silver.withColumnRenamed(\"lastchangedatetime\", \"pr_lastchangedatetime\")\n",
    "\n",
    "df_gold = (\n",
    "    pr_silver.alias(\"pr\")\n",
    "    .join(\n",
    "        po_silver.alias(\"po\"),\n",
    "        (F.col(\"pr_purchaserequisition\") == F.col(\"po_purchaserequisition\")) &\n",
    "        (F.col(\"pr_itemnumber\") == F.col(\"po_itemnumber\")),\n",
    "        \"full\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Note: After the join, columns are accessible as:\n",
    "#   - \"purchaserequisition\", \"pr_itemnumber\", \"pr_creationdate\", ...\n",
    "#   - \"purchaseorder\", \"po_createdon\", \"po_approvaldate\", ...\n",
    "# BUT because we used aliases, Spark will auto-resolve the final column names.\n",
    "# To be explicit and avoid confusion we can reference them without alias prefixes\n",
    "# where Spark has flattened them.\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. COMPUTE PR-to-PO AGEING (CALENDAR DAYS)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_to_po_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"pr_creationdate\").isNotNull()) &\n",
    "        (F.col(\"po_createdon\").isNotNull()) &\n",
    "        (F.col(\"pr_approvalstatus\") == \"Approved\"),\n",
    "        F.when(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_creationdate, po_createdon, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('Sat', 'Sun')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\") < 0,\n",
    "            0\n",
    "        ).otherwise(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_creationdate, po_createdon, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('Sat', 'Sun')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. SLA BREACH FLAG (EXISTING: PR→PO > 5 DAYS)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_to_po_ageing\") > 5, \"YES\")\n",
    "     .when(F.col(\"pr_to_po_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. PR CYCLE TIME (BUSINESS DAYS) & SLA BREACH > 2 DAYS\n",
    "#     PR cycle = pr_creationdate → pr_approveddate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_cycle_time_bd\",\n",
    "    F.expr(\"\"\"\n",
    "        CASE\n",
    "          WHEN pr.pr_creationdate IS NOT NULL\n",
    "               AND pr.pr_approveddate IS NOT NULL THEN\n",
    "            aggregate(\n",
    "              filter(\n",
    "                sequence(pr_creationdate, pr_approveddate, interval 1 day),\n",
    "                d -> date_format(d, 'E') NOT IN ('Sat', 'Sun')\n",
    "              ),\n",
    "              0,\n",
    "              (acc, x) -> acc + 1\n",
    "            )\n",
    "          ELSE NULL\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_cycle_time_bd\") > 2, \"YES\")\n",
    "     .when(F.col(\"pr_cycle_time_bd\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. PO CYCLE TIME (BUSINESS DAYS) & SLA BREACH > 2 DAYS\n",
    "#     PO cycle = po_createdon → po_approvaldate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_cycle_time_bd\",\n",
    "    F.expr(\"\"\"\n",
    "        CASE\n",
    "          WHEN po.po_createdon IS NOT NULL\n",
    "               AND po.po_approvaldate IS NOT NULL THEN\n",
    "            aggregate(\n",
    "              filter(\n",
    "                sequence(to_date(po_createdon), po_approvaldate, interval 1 day),\n",
    "                d -> date_format(d, 'E') NOT IN ('Sat', 'Sun')\n",
    "              ),\n",
    "              0,\n",
    "              (acc, x) -> acc + 1\n",
    "            )\n",
    "          ELSE NULL\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"po_cycle_time_bd\") > 2, \"YES\")\n",
    "     .when(F.col(\"po_cycle_time_bd\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. PO-ONLY AND PR-ONLY HANDLING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"record_type\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) & (F.col(\"purchaseorder\").isNotNull()),\n",
    "        \"PR_PO_MATCHED\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) & (F.col(\"purchaseorder\").isNull()),\n",
    "        \"PR_ONLY\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"prpr_purchaserequisition\").isNull()) & (F.col(\"po.purchaseorder\").isNotNull()),\n",
    "        \"PO_ONLY\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 10. ADD GOLD LOAD DATE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\"gold_load_date\", F.current_date()) \\\n",
    "                 .withColumn(\"gold_load_timestamp\", F.current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 11. WRITE GOLD TABLE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "gold_table = \"abc.abc_dw_gold.abc_dw_gl_procument_cycle_tine\"\n",
    "\n",
    "(\n",
    "    df_gold.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(\"Gold Layer Loaded Successfully with business-day PR & PO cycle SLA flags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec71b0e-0f86-4b52-9fe9-8692727c9ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7050310056558027>, line 228\u001B[0m\n",
       "\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# 9. WRITE GOLD TABLE\u001B[39;00m\n",
       "\u001B[1;32m    220\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m    222\u001B[0m gold_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabc.abc_dw_gold.abc_dw_gl_pr_po_kpi\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    224\u001B[0m (\n",
       "\u001B[1;32m    225\u001B[0m     df_gold\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    226\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 228\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(gold_table)\n",
       "\u001B[1;32m    229\u001B[0m )\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGold Layer Loaded Successfully with ALL KPIs in business days (incl. PR approval ageing).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n",
       "\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    739\u001B[0m )\n",
       "\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnknownException\u001B[0m: (java.lang.IllegalStateException) \n",
       "        operationId: 1d0f8c0b-8d27-4833-b6e4-7b008c42ec29 with status ReadyForExecution\n",
       "        is not within statuses List(Finished, Failed, Canceled) for event Closed\n",
       "        \n",
       "\n",
       "JVM stacktrace:\n",
       "java.lang.IllegalStateException\n",
       "\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:414)\n",
       "\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:351)\n",
       "\tat org.apache.spark.sql.connect.service.ExecuteHolder.cleanup(ExecuteHolder.scala:346)\n",
       "\tat org.apache.spark.sql.connect.service.ExecuteHolder.close(ExecuteHolder.scala:321)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.removeExecuteHolder(SparkConnectExecutionManager.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.reattachExecuteHolder(SparkConnectExecutionManager.scala:293)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectReattachExecuteHandler.handle(SparkConnectReattachExecuteHandler.scala:67)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.reattachExecute(SparkConnectService.scala:193)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:886)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnknownException",
        "evalue": "(java.lang.IllegalStateException) \n        operationId: 1d0f8c0b-8d27-4833-b6e4-7b008c42ec29 with status ReadyForExecution\n        is not within statuses List(Finished, Failed, Canceled) for event Closed\n        \n\nJVM stacktrace:\njava.lang.IllegalStateException\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:414)\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:351)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.cleanup(ExecuteHolder.scala:346)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.close(ExecuteHolder.scala:321)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.removeExecuteHolder(SparkConnectExecutionManager.scala:237)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.reattachExecuteHolder(SparkConnectExecutionManager.scala:293)\n\tat org.apache.spark.sql.connect.service.SparkConnectReattachExecuteHandler.handle(SparkConnectReattachExecuteHandler.scala:67)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.reattachExecute(SparkConnectService.scala:193)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:886)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>UnknownException</span>: (java.lang.IllegalStateException) \n        operationId: 1d0f8c0b-8d27-4833-b6e4-7b008c42ec29 with status ReadyForExecution\n        is not within statuses List(Finished, Failed, Canceled) for event Closed\n        \n\nJVM stacktrace:\njava.lang.IllegalStateException\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:414)\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:351)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.cleanup(ExecuteHolder.scala:346)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.close(ExecuteHolder.scala:321)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.removeExecuteHolder(SparkConnectExecutionManager.scala:237)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.reattachExecuteHolder(SparkConnectExecutionManager.scala:293)\n\tat org.apache.spark.sql.connect.service.SparkConnectReattachExecuteHandler.handle(SparkConnectReattachExecuteHandler.scala:67)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.reattachExecute(SparkConnectService.scala:193)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:886)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "java.lang.IllegalStateException\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:414)\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:351)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.cleanup(ExecuteHolder.scala:346)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.close(ExecuteHolder.scala:321)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.removeExecuteHolder(SparkConnectExecutionManager.scala:237)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.reattachExecuteHolder(SparkConnectExecutionManager.scala:293)\n\tat org.apache.spark.sql.connect.service.SparkConnectReattachExecuteHandler.handle(SparkConnectReattachExecuteHandler.scala:67)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.reattachExecute(SparkConnectService.scala:193)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:886)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-7050310056558027>, line 228\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# 9. WRITE GOLD TABLE\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    222\u001B[0m gold_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabc.abc_dw_gold.abc_dw_gl_pr_po_kpi\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    224\u001B[0m (\n\u001B[1;32m    225\u001B[0m     df_gold\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 228\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(gold_table)\n\u001B[1;32m    229\u001B[0m )\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGold Layer Loaded Successfully with ALL KPIs in business days (incl. PR approval ageing).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    739\u001B[0m )\n\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnknownException\u001B[0m: (java.lang.IllegalStateException) \n        operationId: 1d0f8c0b-8d27-4833-b6e4-7b008c42ec29 with status ReadyForExecution\n        is not within statuses List(Finished, Failed, Canceled) for event Closed\n        \n\nJVM stacktrace:\njava.lang.IllegalStateException\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:414)\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:351)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.cleanup(ExecuteHolder.scala:346)\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.close(ExecuteHolder.scala:321)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.removeExecuteHolder(SparkConnectExecutionManager.scala:237)\n\tat org.apache.spark.sql.connect.service.SparkConnectExecutionManager.reattachExecuteHolder(SparkConnectExecutionManager.scala:293)\n\tat org.apache.spark.sql.connect.service.SparkConnectReattachExecuteHandler.handle(SparkConnectReattachExecuteHandler.scala:67)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.reattachExecute(SparkConnectService.scala:193)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:886)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LOAD SILVER TABLES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Rename silver_load_timestamp in both source tables before the join\n",
    "po_silver = (\n",
    "    spark.table(\"abc.abc_dw_silver.abc_dw_sl_pur_ord\")\n",
    "    .withColumnRenamed(\"recordskipindicator\", \"po_recordskipindicator\")\n",
    "    .withColumnRenamed(\"silver_load_date\", \"po_silver_load_date\")\n",
    "    .withColumnRenamed(\"silver_load_timestamp\", \"po_silver_load_timestamp\")\n",
    ")\n",
    "pr_silver = (\n",
    "    spark.table(\"abc.abc_dw_silver.abc_dw_sl_pr_req\")\n",
    "    .withColumnRenamed(\"recordskipindicator\", \"pr_recordskipindicator\")\n",
    "    .withColumnRenamed(\"silver_load_date\", \"pr_silver_load_date\")\n",
    "    .withColumnRenamed(\"silver_load_timestamp\", \"pr_silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "# Continue with the rest of your code unchanged\n",
    "# ---------------------------------------------------------\n",
    "# 2. FULL OUTER JOIN PR + PO\n",
    "#    Join keys:\n",
    "#      PR : pr_purchaserequisition, pr_itemnumber\n",
    "#      PO : po_purchaserequisition, po_itemnumber\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 2. FULL OUTER JOIN PR + PO\n",
    "df_gold = (\n",
    "    pr_silver.alias(\"pr\")\n",
    "    .join(\n",
    "        po_silver.alias(\"po\"),\n",
    "        (F.col(\"pr.pr_purchaserequisition\") == F.col(\"po.po_purchaserequisition\")) &\n",
    "        (F.col(\"pr.pr_itemnumber\") == F.col(\"po.po_itemnumber\")),\n",
    "        \"full\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# After the join, column names are flattened:\n",
    "#   pr_purchaserequisition, pr_itemnumber, pr_creationdate, pr_approveddate, ...\n",
    "#   po_purchaserequisition, po_itemnumber, po_createdon, po_approvaldate, ...\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PR→PO AGEING (BUSINESS DAYS)\n",
    "#     pr_approvaldate → po_createdon\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_to_po_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"pr_approveddate\").isNotNull()) &\n",
    "        (F.col(\"po_createdon\").isNotNull()) &\n",
    "        (F.col(\"pr_approvalstatus\") == \"Approved\"),\n",
    "        F.when(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_approveddate, po_createdon, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\") < 0,\n",
    "            0\n",
    "        ).otherwise(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_approveddate, po_createdon, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. PR APPROVAL AGEING (BUSINESS DAYS)\n",
    "#     pr_creationdate → pr_approveddate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_approval_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_creationdate\").isNotNull()) &\n",
    "        (F.col(\"pr_approveddate\").isNotNull()),\n",
    "        F.when(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_creationdate, pr_approveddate, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\") < 0,\n",
    "            0\n",
    "        ).otherwise(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(pr_creationdate, pr_approveddate, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. PO APPROVAL AGEING (BUSINESS DAYS)\n",
    "#     po_createdon → po_approvaldate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_approval_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"po_createdon\").isNotNull()) &\n",
    "        (F.col(\"po_approvaldate\").isNotNull()),\n",
    "        F.when(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(po_createdon, po_approvaldate, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\") < 0,\n",
    "            0\n",
    "        ).otherwise(\n",
    "            F.expr(\"\"\"\n",
    "                aggregate(\n",
    "                  filter(\n",
    "                    sequence(po_createdon, po_approvaldate, interval 1 day),\n",
    "                    d -> date_format(d, 'E') NOT IN ('fri','Sat')\n",
    "                  ),\n",
    "                  0,\n",
    "                  (acc, x) -> acc + 1\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. SLA BREACH FLAGS (BUSINESS DAYS)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# PR→PO SLA: > 5 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_to_po_ageing\") > 5, \"YES\")\n",
    "     .when(F.col(\"pr_to_po_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# PR cycle SLA: > 2 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_approval_ageing\") > 2, \"YES\")\n",
    "     .when(F.col(\"pr_approval_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# PO cycle SLA: > 2 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"po_approval_ageing\") > 2, \"YES\")\n",
    "     .when(F.col(\"po_approval_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. RECORD TYPE CLASSIFICATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"record_type\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()),\n",
    "        \"PR_PO_MATCHED\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNull()),\n",
    "        \"PR_ONLY\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()),\n",
    "        \"PO_ONLY\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. GOLD METADATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\"gold_load_date\", F.current_date()) \\\n",
    "                 .withColumn(\"gold_load_timestamp\", F.current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. WRITE GOLD TABLE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "gold_table = \"abc.abc_dw_gold.abc_dw_gl_pr_po_kpi\"\n",
    "\n",
    "(\n",
    "    df_gold.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(\"Gold Layer Loaded Successfully with ALL KPIs in business days (incl. PR approval ageing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b74ca9-be30-4ee5-8628-4ded3980fa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Layer Loaded Successfully with ALL KPIs in business days (Sun–Thu).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LOAD SILVER TABLES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "po_silver = (\n",
    "    spark.table(\"abc.abc_dw_silver.abc_dw_sl_pur_ord\")\n",
    "    .withColumnRenamed(\"recordskipindicator\", \"po_recordskipindicator\")\n",
    "    .withColumnRenamed(\"silver_load_date\", \"po_silver_load_date\")\n",
    "    .withColumnRenamed(\"silver_load_timestamp\", \"po_silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "pr_silver = (\n",
    "    spark.table(\"abc.abc_dw_silver.abc_dw_sl_pr_req\")\n",
    "    .withColumnRenamed(\"recordskipindicator\", \"pr_recordskipindicator\")\n",
    "    .withColumnRenamed(\"silver_load_date\", \"pr_silver_load_date\")\n",
    "    .withColumnRenamed(\"silver_load_timestamp\", \"pr_silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FULL OUTER JOIN PR + PO\n",
    "#    Join keys:\n",
    "#      PR : pr_purchaserequisition, pr_itemnumber\n",
    "#      PO : po_purchaserequisition, po_itemnumber\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = (\n",
    "    pr_silver.alias(\"pr\")\n",
    "    .join(\n",
    "        po_silver.alias(\"po\"),\n",
    "        (F.col(\"pr.pr_purchaserequisition\") == F.col(\"po.po_purchaserequisition\")) &\n",
    "        (F.col(\"pr.pr_itemnumber\") == F.col(\"po.po_itemnumber\")),\n",
    "        \"full\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# After join:\n",
    "#   pr_purchaserequisition, pr_itemnumber, pr_creationdate, pr_approveddate, ...\n",
    "#   po_purchaserequisition, po_itemnumber, po_createdon, po_approvaldate, ...\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PR→PO AGEING (BUSINESS DAYS, Sun–Thu)\n",
    "#     pr_approveddate → po_createdon\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_to_po_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"pr_approveddate\").isNotNull()) &\n",
    "        (F.col(\"po_createdon\").isNotNull()) &\n",
    "        (F.col(\"pr_approvalstatus\") == \"Approved\") &\n",
    "        (F.col(\"po_createdon\") >= F.col(\"pr_approveddate\")),\n",
    "        F.size(\n",
    "            F.expr(\"\"\"\n",
    "                filter(\n",
    "                  sequence(pr_approveddate, po_createdon),\n",
    "                  d -> dayofweek(d) BETWEEN 1 AND 5\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. PR APPROVAL AGEING (BUSINESS DAYS, Sun–Thu)\n",
    "#     pr_creationdate → pr_approveddate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_approval_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_creationdate\").isNotNull()) &\n",
    "        (F.col(\"pr_approveddate\").isNotNull()) &\n",
    "        (F.col(\"pr_approveddate\") >= F.col(\"pr_creationdate\")),\n",
    "        F.size(\n",
    "            F.expr(\"\"\"\n",
    "                filter(\n",
    "                  sequence(pr_creationdate, pr_approveddate),\n",
    "                  d -> dayofweek(d) BETWEEN 1 AND 5\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. PO APPROVAL AGEING (BUSINESS DAYS, Sun–Thu)\n",
    "#     po_createdon → po_approvaldate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_approval_ageing\",\n",
    "    F.when(\n",
    "        (F.col(\"po_createdon\").isNotNull()) &\n",
    "        (F.col(\"po_approvaldate\").isNotNull()) &\n",
    "        (F.col(\"po_approvaldate\") >= F.col(\"po_createdon\")),\n",
    "        F.size(\n",
    "            F.expr(\"\"\"\n",
    "                filter(\n",
    "                  sequence(po_createdon, po_approvaldate),\n",
    "                  d -> dayofweek(d) BETWEEN 1 AND 5\n",
    "                )\n",
    "            \"\"\")\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. SLA BREACH FLAGS (BUSINESS DAYS)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# PR→PO SLA: > 5 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_to_po_ageing\") > 5, \"YES\")\n",
    "     .when(F.col(\"pr_to_po_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# PR cycle SLA: > 2 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"pr_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"pr_approval_ageing\") > 2, \"YES\")\n",
    "     .when(F.col(\"pr_approval_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# PO cycle SLA: > 2 business days\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"po_cycle_sla_breach_flag\",\n",
    "    F.when(F.col(\"po_approval_ageing\") > 2, \"YES\")\n",
    "     .when(F.col(\"po_approval_ageing\").isNull(), None)\n",
    "     .otherwise(\"NO\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. RECORD TYPE CLASSIFICATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"record_type\",\n",
    "    F.when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()),\n",
    "        \"PR_PO_MATCHED\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNotNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNull()),\n",
    "        \"PR_ONLY\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"pr_purchaserequisition\").isNull()) &\n",
    "        (F.col(\"po_purchaserequisition\").isNotNull()),\n",
    "        \"PO_ONLY\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. GOLD METADATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_gold = df_gold.withColumn(\"gold_load_date\", F.current_date()) \\\n",
    "                 .withColumn(\"gold_load_timestamp\", F.current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. WRITE GOLD TABLE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "gold_table = \"abc.abc_dw_gold.abc_dw_gl_pr_po_kpi\"\n",
    "\n",
    "(\n",
    "    df_gold.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(\"Gold Layer Loaded Successfully with ALL KPIs in business days (Sun–Thu).\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}